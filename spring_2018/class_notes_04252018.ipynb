{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying Text . . . \n",
    "\n",
    "Using the NTK Naive Bayes classifier (by way of Textblob), which is slow, but [simple and well-documented](http://textblob.readthedocs.io/en/dev/classifiers.html).  The [NLTK documentation](http://www.nltk.org/book/ch06.html) is also useful.\n",
    "\n",
    "*The Programming Historian* has [a useful survery of clustering and classification](https://programminghistorian.org/lessons/naive-bayesian#machine-learning).\n",
    "\n",
    "But, unfortunately, the NTLK/Textblob Naive Bayes classifier is slow.  Very, very slow.  \n",
    "\n",
    "*GeeksForGeeks* has [an overview of sklearn classification](https://www.geeksforgeeks.org/multiclass-classification-using-scikit-learn/), which includes code snippets.  We'll use [an sklearn Naive Bayes classifier](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html#sklearn.naive_bayes.BernoulliNB.predict), since it's so fast.\n",
    "\n",
    "Hoyt Long and Richard So, [\"Literary Pattern Recognition: Modernism between Close Reading and Machine Learning\"](https://lucian.uchicago.edu/blogs/literarynetworks/files/2015/12/LONG_SO_CI.pdf), *Critical Inquiry*, Vol. 42, No. 2 (Winter 2016), pp. 235-267.  And [a 20 minute video discussing the article](https://criticalinquiry.uchicago.edu/hoyt_long_and_richard_so_on_close_reading_machine_learning_and_patterns_in/).  **Done with naive bayes.**\n",
    "\n",
    "## . . . Using Data from the Muncie Public Library\n",
    "\n",
    "[What Middletown Read](http://www.bsu.edu/libraries/wmr/).  175,000 transactions; ~ 1890 to 1900; ~6,000 readers with census data attached (age, gender); plain text for 60% of the transactions.\n",
    "\n",
    "[Gendered reading in the Muncie Public Library](https://talus.artsci.wustl.edu/ageGenderCharts/examples/muncieAuthors.html)\n",
    "\n",
    "**Very gendered reading by boys and girls.** Girls tended to read one set of books, boys to read another; i.e., a lot of books readership was ~ 70% boys, or 70% girls.\n",
    "\n",
    "Can we distingush content differences between \"boy\" books and \"girl\" books?  How much of an exception are books by Horatio Alger?  Are best sellers more like \"boy\" books or \"girl\" books?\n",
    "\n",
    "## Why Muncie here?  \"Labeled Data\"\n",
    "\n",
    "Because classification requires [**labeled data**](https://en.wikipedia.org/wiki/Labeled_data) for training and testing.  We can use a trained and tested classifier to predict the label for unlabeled data; however, at least some of our data needs to be labeled before we can start.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drwxrwxr-x 2 spenteco spenteco  4096 Jun  7  2017 corpora/muncie/alger\n",
      "drwxrwxr-x 2 spenteco spenteco 20480 Apr 24 15:54 corpora/muncie/alger_boys\n",
      "drwxrwxr-x 2 spenteco spenteco 16384 Apr 24 09:11 corpora/muncie/best_sellers\n",
      "drwxrwxr-x 2 spenteco spenteco 16384 Apr 24 09:12 corpora/muncie/boys\n",
      "drwxrwxr-x 2 spenteco spenteco 12288 Apr 24 09:13 corpora/muncie/girls\n",
      "\n",
      "69\n",
      "88\n",
      "48\n",
      "105\n",
      "116\n"
     ]
    }
   ],
   "source": [
    "!ls -ld corpora/muncie/*\n",
    "!echo ''\n",
    "!ls -1 corpora/muncie/boys/* | wc -l\n",
    "!ls -1 corpora/muncie/girls/* | wc -l\n",
    "!ls -1 corpora/muncie/alger/* | wc -l\n",
    "!ls -1 corpora/muncie/best_sellers/* | wc -l\n",
    "!ls -1 corpora/muncie/alger_boys/* | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Routines to load corpora and texts\n",
    "\n",
    "Notice that I'm not doing any complicated NLP processing; I'm just doing regex tokenization, and dropping stopwords.\n",
    "\n",
    "The \"class Text\" is probably an unnecessary elaboration; however, when I started this notebook, I wasn't sure how much complexity I was going to need, so I wanted to come up with some way to \"black box\" that complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading folder alger\n",
      "loading folder best_sellers\n",
      "loading folder boys\n",
      "loading folder girls\n",
      "loading folder alger_boys\n",
      "\n",
      "boys 69\n",
      "best_sellers 105\n",
      "girls 88\n",
      "alger_boys 116\n",
      "alger 48\n",
      "\n",
      "[u'away', u'camp', u'possible', u'escape', u'became', u'discovered', u'frank', u'without', u'waiting', u'receive', u'congratulations', u'mate', u'looked', u'upon', u'escape', u'certain', u'thing', u'threw', u'hands', u'knees', u'moved', u'slowly', u'across', u'field', u'extended', u'mile', u'back', u'cabin', u'must', u'crossed', u'could', u'reach', u'woods', u'progress', u'slow', u'laborious', u'two', u'hours', u'reached', u'road', u'ran', u'direction', u'supposed', u'river', u'lie', u'seen', u'pickets', u'feeling', u'quite', u'certain', u'outside', u'lines', u'arose', u'feet', u'commenced', u'running', u'top', u'speed', u'road', u'ran', u'thick', u'woods', u'difficulty', u'following', u'moon', u'shining', u'brightly', u'daylight', u'arrived', u'mississippi', u'pleasant', u'sight', u'eyes', u'uttered', u'shout', u'joy', u'found', u'standing', u'banks', u'spirits', u'fell', u'glancing', u'river', u'far', u'eyes', u'could', u'reach', u'could', u'see', u'vessel', u'kind', u'sight', u'yet', u'journey', u'end', u'might', u'gun', u'boat', u'close', u'hid', u'behind', u'one', u'numerous', u'points', u'stretched', u'river', u'might', u'one', u'within', u'hundred', u'miles', u'must', u'linger', u'however', u'free', u'pursuit', u'safe', u'board', u'vessel', u'sorrowfully', u'bent', u'steps', u'river', u'listening', u'sounds', u'pursuit', u'eagerly', u'watching', u'signs', u'approaching', u'steamer', u'day', u'wore', u'away', u'fugitives', u'began', u'feel', u'effects', u'hunger', u'halted', u'debating', u'upon', u'means', u'used', u'procuring', u'food', u'joy', u'discovered', u'smoke', u'around', u'bend', u'half', u'hour', u'transport', u'loaded', u'soldiers', u'appeared', u'sight', u'commenced', u'waving', u'hats', u'attract', u'attention', u'board', u'evidently', u'saw', u'suspicious', u'plan', u'rebels', u'decoy', u'shore', u'turned', u'toward', u'opposite', u'bank', u'think', u'ought', u'see', u'us', u'said', u'frank', u'commenced', u'shouting', u'top', u'lungs', u'moment', u'afterward', u'puff', u'smoke', u'arose', u'forecastle', u'twelve', u'pounder', u'shot', u'plowed', u'water', u'lodged', u'bank', u'feet', u'evident']\n",
      "\n",
      "Done! 28.9061729908\n"
     ]
    }
   ],
   "source": [
    "import glob, codecs, re, random, time\n",
    "from nltk.corpus import stopwords\n",
    "        \n",
    "sw = set(stopwords.words('english'))\n",
    "\n",
    "class Text():\n",
    "    \n",
    "    def __init__(self, parm_path_to_file):\n",
    "    \n",
    "        self.author_title = parm_path_to_file.split('/')[-1].replace('.txt', '')\n",
    "        \n",
    "        self.raw_text = re.sub('\\s+', ' ', codecs.open(parm_path_to_file, 'r', encoding='utf-8').read())\n",
    "        \n",
    "        self.tokens = []\n",
    "        for t in re.split('[^a-z]', self.raw_text.lower()):\n",
    "            if t > '' and t not in sw:\n",
    "                self.tokens.append(t)\n",
    "        \n",
    "    def get_random_slice(self, parm_slice_length):\n",
    "        \n",
    "        last_possible_starting_position = len(self.tokens) - parm_slice_length - 1\n",
    "        \n",
    "        starting_position = random.randint(0, last_possible_starting_position)\n",
    "        ending_position = starting_position + parm_slice_length\n",
    "        \n",
    "        token_slice = self.tokens[starting_position: ending_position]\n",
    "        \n",
    "        return token_slice\n",
    "        \n",
    "#  --------------------------------------------------------------------------------\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "subcorpora_folders = ['alger', 'best_sellers', 'boys', 'girls', 'alger_boys']\n",
    "\n",
    "my_corpora = {}\n",
    "\n",
    "for folder in subcorpora_folders:\n",
    "    \n",
    "    print 'loading folder', folder\n",
    "    \n",
    "    my_corpora[folder] = []   \n",
    "        \n",
    "    for path_to_file in glob.glob('corpora/muncie/' + folder + '/*.txt'):\n",
    "        \n",
    "        my_corpora[folder].append(Text(path_to_file))\n",
    "\n",
    "print\n",
    "for k, v in my_corpora.iteritems():\n",
    "    print k, len(v)\n",
    "    \n",
    "print\n",
    "print my_corpora['boys'][0].get_random_slice(200)\n",
    "\n",
    "stop_time = time.time()\n",
    "    \n",
    "print\n",
    "print 'Done!', (stop_time - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic use of the classifiers\n",
    "\n",
    "### Pull a set of data to feed into the classifiers\n",
    "\n",
    "NLTK and sklearn want different inputs, hences the different \"samples\" lists.  NLTK wants a list of tuples, one item in the list per text; the tuples have two parts: the text as a string, and the label as a string.  sklearn wants a dense matrix, so we're going to prepare \"samples\" suitable for passing into gensim, etc; these gensim->sklearn \"samples\" are a list of lists of string tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "training_sources = []\n",
    "training_samples_nltk = []\n",
    "training_samples_sklearn = []\n",
    "training_labels = []\n",
    "\n",
    "testing_sources = []\n",
    "testing_samples_nltk = []\n",
    "testing_samples_sklearn = []\n",
    "testing_labels = []\n",
    "\n",
    "for folder in ['boys', 'girls']:\n",
    "    for text in my_corpora[folder]:\n",
    "        \n",
    "        training_slice = text.get_random_slice(1000)\n",
    "    \n",
    "        training_sources.append(text.author_title)\n",
    "        training_samples_nltk.append((' '.join(training_slice) , folder))\n",
    "        training_samples_sklearn.append(training_slice)\n",
    "        training_labels.append(folder)\n",
    "        \n",
    "        testing_slice = text.get_random_slice(1000)\n",
    "    \n",
    "        testing_sources.append(text.author_title)\n",
    "        testing_samples_nltk.append((' '.join(testing_slice) , folder))\n",
    "        testing_samples_sklearn.append(testing_slice)\n",
    "        testing_labels.append(folder)\n",
    "        \n",
    "#print\n",
    "#print 'training_sources', training_sources[:10]\n",
    "#print\n",
    "#print 'training_samples_nltk', training_samples_nltk[:10]\n",
    "#print\n",
    "#print 'training_samples_sklearn', training_samples_sklearn[:10]\n",
    "#print\n",
    "#print 'training_labels', training_labels[:10]\n",
    "\n",
    "print\n",
    "print 'Done!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and test a Textblob Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classifier training 145.463336945\n",
      "accuracy 0.968152866242\n",
      "classifier training 196.911050797\n",
      "\n",
      "Most Informative Features\n",
      "          contains(guns) = True             boys : girls  =     19.1 : 1.0\n",
      "          contains(camp) = True             boys : girls  =     14.0 : 1.0\n",
      "        contains(lovely) = True            girls : boys   =     13.9 : 1.0\n",
      "          contains(papa) = True            girls : boys   =     13.9 : 1.0\n",
      "       contains(advance) = True             boys : girls  =     13.1 : 1.0\n",
      "       contains(flowers) = True            girls : boys   =     12.3 : 1.0\n",
      "       contains(capture) = True             boys : girls  =     12.3 : 1.0\n",
      "          contains(game) = True             boys : girls  =     11.4 : 1.0\n",
      "         contains(boats) = True             boys : girls  =     11.4 : 1.0\n",
      "         contains(fired) = True             boys : girls  =     11.4 : 1.0\n",
      "         contains(avoid) = True             boys : girls  =     11.4 : 1.0\n",
      "         contains(rifle) = True             boys : girls  =     11.4 : 1.0\n",
      "        contains(heaven) = True            girls : boys   =     11.3 : 1.0\n",
      "         contains(yards) = True             boys : girls  =     10.9 : 1.0\n",
      "         contains(cabin) = True             boys : girls  =     10.6 : 1.0\n",
      "         contains(heels) = True             boys : girls  =      9.7 : 1.0\n",
      "      contains(directed) = True             boys : girls  =      9.7 : 1.0\n",
      "           contains(tea) = True            girls : boys   =      9.2 : 1.0\n",
      "         contains(armed) = True             boys : girls  =      8.9 : 1.0\n",
      "     contains(mountains) = True             boys : girls  =      8.9 : 1.0\n",
      "     contains(commander) = True             boys : girls  =      8.9 : 1.0\n",
      "      contains(engineer) = True             boys : girls  =      8.9 : 1.0\n",
      "       contains(hunting) = True             boys : girls  =      8.4 : 1.0\n",
      "            contains(dr) = True            girls : boys   =      8.1 : 1.0\n",
      "        contains(garden) = True            girls : boys   =      8.1 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import time, random\n",
    "from textblob.classifiers import NaiveBayesClassifier\n",
    "\n",
    "# -------------------------------------------------------------------------------\n",
    "# TRAIN BY PASSING IN ONE SET OF LABELED DATA; THIS RESULTS IN \"cl\", WHICH IS A\n",
    "# MODEL WHICH RELATES WORDS IN THE SAMPLES TO THE LABELS IN THE SAMPLES.\n",
    "# -------------------------------------------------------------------------------\n",
    "\n",
    "start_time = time.time()\n",
    "        \n",
    "cl = NaiveBayesClassifier(training_samples_nltk)\n",
    "\n",
    "stop_time = time.time()\n",
    "\n",
    "print 'classifier training', (stop_time - start_time)\n",
    "\n",
    "# -------------------------------------------------------------------------------\n",
    "# TAKE ANOTHER SET OF LABELED DATA, AND TEST TO SEE IF THE CLASSIFIER GETS THE\n",
    "# RIGHT ANSWER (I.E., DO THE WORDS IN THE TESTING SAMPLES LEAD THE THE LABELS IN\n",
    "# THE TESTING SAMPLES?)\n",
    "# -------------------------------------------------------------------------------\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "accuracy = cl.accuracy(testing_samples_nltk)\n",
    "\n",
    "print 'accuracy', accuracy\n",
    "\n",
    "stop_time = time.time()\n",
    "\n",
    "print 'classifier training', (stop_time - start_time)\n",
    "\n",
    "# -------------------------------------------------------------------------------\n",
    "# WHAT WORDS ARE MOST USEFUL IN DIFFERENTIATING BETWEEN THE LABELS?\n",
    "# -------------------------------------------------------------------------------\n",
    "\n",
    "print\n",
    "print cl.show_informative_features(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sklearn does not provide an easy way . . . \n",
    "\n",
    ". . . to get at the most informative features.  This, if not quite right, is a reasonable substitute.\n",
    "\n",
    "NLTK provides the word-label probabilities in a decimal format, which is easy to understand.  sklearn provides the log probabilities, which I find much harder to understand.  Doug Knox, my collegue in the HDW, suggested this bit of math to convert log probabilites to NLTK-like decimal-format probabilities.  The conversion doesn't preproduce the NLTK results, so **more work is needed here**.  But this seems provisionally workable . . . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def print_most_informative_sklearn(classifier, n_to_list):\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for a in range(0, len(classifier.feature_log_prob_[0])):\n",
    "\n",
    "            class_0 = (math.e**classifier.feature_log_prob_[0][a])\n",
    "            class_1 = (math.e**classifier.feature_log_prob_[1][a])\n",
    "\n",
    "            if class_0 > class_1:\n",
    "                results.append([(class_0 / class_1), \n",
    "                                [dictionary[a].ljust(20), 'boy : girl', '\\t', \n",
    "                                     '%.1f' % (class_0 / class_1), ':', str(1.0)]])\n",
    "            else:\n",
    "                results.append([(class_1 / class_0), \n",
    "                                [dictionary[a].ljust(20), 'girl : boy', '\\t', \n",
    "                                     '%.1f' % (class_1 / class_0), ':', str(1.0)]])\n",
    "\n",
    "    results.sort(reverse=True)\n",
    "\n",
    "    for r in results[:n_to_list]:\n",
    "        print ' '.join(r[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run sklearn Naive Bayes classifier\n",
    "\n",
    "Note that we first go through gensim to get a dense corpus (\"training_matrix\" and \"testing_matrix\").\n",
    "\n",
    "Lines like this:\n",
    "\n",
    "    training_matrix = training_matrix.T\n",
    "   \n",
    "is [some unexplained magic](https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.T.html).  Unexplained because we didn't dive into numpy.  **Bottom line**: the array from gensim.matutils.corpus2dense (which is a term-document matrix) needs to be \"turned\" so it's a document-term matrix.\n",
    "\n",
    "Note how much faster this is (<0.1 second) vs NLTK/Textblob (5 or 6 minutes)?   Note that the results, while comparable to NTLK/Textblob, are not exactly the same.\n",
    "\n",
    "Also, I'm using the BernoulliNB classifier, and not the GaussianNB classifier.  Why?  The BernoulliNB provides the data neceessary to list the most informative features; the GaussianNB does not.  What's the difference between GaussianNB and BernoulliNB?  It's not clear to me: it seems to be a question of the shape of the feature distributions expected by each . . . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(dictionary) 22596\n",
      "training_matrix (157, 22596)\n",
      "testing_matrix (157, 22596)\n",
      "classifier trained 0.0401010513306\n",
      "accuracy 0.955414012739\n",
      "testing done 0.0406680107117\n",
      "\n",
      "prisoner             boy : girl \t 24.1 : 1.0\n",
      "captured             boy : girl \t 20.3 : 1.0\n",
      "mamma                girl : boy \t 17.4 : 1.0\n",
      "vessels              boy : girl \t 15.2 : 1.0\n",
      "united               boy : girl \t 15.2 : 1.0\n",
      "halt                 boy : girl \t 15.2 : 1.0\n",
      "elsie                girl : boy \t 15.0 : 1.0\n",
      "darling              girl : boy \t 15.0 : 1.0\n",
      "guns                 boy : girl \t 14.6 : 1.0\n",
      "tremendous           boy : girl \t 13.9 : 1.0\n",
      "blankets             boy : girl \t 13.9 : 1.0\n",
      "pray                 girl : boy \t 13.4 : 1.0\n",
      "warriors             boy : girl \t 12.7 : 1.0\n",
      "u                    boy : girl \t 12.7 : 1.0\n",
      "range                boy : girl \t 12.7 : 1.0\n",
      "ladder               boy : girl \t 12.7 : 1.0\n",
      "hunt                 boy : girl \t 12.7 : 1.0\n",
      "f                    boy : girl \t 12.7 : 1.0\n",
      "dense                boy : girl \t 12.7 : 1.0\n",
      "lap                  girl : boy \t 11.8 : 1.0\n"
     ]
    }
   ],
   "source": [
    "import time, random\n",
    "from gensim import corpora, matutils\n",
    "#from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# THE BY NOW FAMILIAR TRANSFORMATION:\n",
    "#     1.  CORPUS AS A LIST OF LIST OF STRINGS (\"training_samples_sklearn\",\n",
    "#         \"testing_samples_sklearn\") TO\n",
    "#     2.  GENSIM DICTIONARY AND DOC2BOW CORPUS\n",
    "#     3.  GENSIM SPARSE MATRIX (I.E, A DOC2BOW CORPUS) TO DENSE MATRIX\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "dictionary = corpora.Dictionary(training_samples_sklearn + testing_samples_sklearn)\n",
    "\n",
    "print 'len(dictionary)', len(dictionary)\n",
    "\n",
    "training_corpus = [dictionary.doc2bow(text) for text in training_samples_sklearn]\n",
    "\n",
    "testing_corpus = [dictionary.doc2bow(text) for text in testing_samples_sklearn]\n",
    "\n",
    "training_matrix = matutils.corpus2dense(training_corpus, len(dictionary))\n",
    "training_matrix = training_matrix.T\n",
    "\n",
    "print 'training_matrix', training_matrix.shape\n",
    "\n",
    "testing_matrix = matutils.corpus2dense(testing_corpus, len(dictionary))\n",
    "testing_matrix = testing_matrix.T\n",
    "\n",
    "print 'testing_matrix', testing_matrix.shape\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# TWO LINES TO TRAIN THE CLASSIFIER\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "classifier = BernoulliNB()\n",
    "classifier.fit(training_matrix, training_labels)\n",
    "\n",
    "stop_time = time.time()\n",
    "\n",
    "print 'classifier trained', (stop_time - start_time)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# ONE LINE TO ASSESS THE ACCURACY OF THE CLASSIFIER . . . \n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "score = classifier.score(testing_matrix, testing_labels)\n",
    "\n",
    "print 'accuracy', score\n",
    "        \n",
    "stop_time = time.time()\n",
    "\n",
    "print 'testing done', (stop_time - start_time)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# . . . AND A CALL TO THE FUNCTION WHICH CONVERTS THE PROBABILITIES, ETC.\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "print\n",
    "print_most_informative_sklearn(classifier, 20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is Alger more like \"boy\" books, or \"girl\" books?  Best sellers?\n",
    "\n",
    "\n",
    "### Routines to 1) get data and 2) train and evaluate a classifier\n",
    "\n",
    "I want these in a separate cell, because I'm going to be doing them a bunch . .  ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time, random\n",
    "from gensim import corpora, matutils\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "def get_some_data(folders, slice_size):\n",
    "\n",
    "    sources = []\n",
    "    samples = []\n",
    "    labels = []\n",
    "\n",
    "    for folder in folders:\n",
    "        for text in my_corpora[folder]:\n",
    "\n",
    "            random_slice = text.get_random_slice(slice_size)\n",
    "\n",
    "            sources.append(text.author_title)\n",
    "            samples.append(random_slice)\n",
    "            labels.append(folder)\n",
    "            \n",
    "    return sources, samples, labels\n",
    "\n",
    "def train_and_test_a_classifier(training_samples, \n",
    "                                training_labels, \n",
    "                                testing_samples, \n",
    "                                testing_labels,\n",
    "                                prediction_samples):\n",
    "        \n",
    "    dictionary = corpora.Dictionary(training_samples + testing_samples + prediction_samples)\n",
    "\n",
    "    training_corpus = [dictionary.doc2bow(text) for text in training_samples]\n",
    "\n",
    "    testing_corpus = [dictionary.doc2bow(text) for text in testing_samples]\n",
    "\n",
    "    training_matrix = matutils.corpus2dense(training_corpus, len(dictionary))\n",
    "    training_matrix = training_matrix.T\n",
    "\n",
    "    testing_matrix = matutils.corpus2dense(testing_corpus, len(dictionary))\n",
    "    testing_matrix = testing_matrix.T\n",
    "    \n",
    "    classifier = BernoulliNB()\n",
    "    classifier.fit(training_matrix, training_labels)\n",
    "\n",
    "    score = classifier.score(testing_matrix, testing_labels)\n",
    "    \n",
    "    return dictionary, classifier, score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alger process\n",
    "\n",
    "The question:  **Is Alger text more like texts favored by boys, or by girls?**\n",
    "\n",
    "1.  Train and test a classifier using \"boys\" and \"girls\" books\n",
    "2.  Ask the classifier to predict the labels for Alger.  Does it think the Alger data is from \"boy\" or \"girl\" books?\n",
    "3.  Repeat.\n",
    "\n",
    "### Unanswered questions\n",
    "\n",
    "I have more girl samples than boy samples.  Does this skew my results?  Or does the accuracy number indicate that I really don't need to worry so much abou that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.94 len(alger_samples) 48 n_girls 42 n_boys 6  -->  girls/n samples 0.88\n",
      "accuracy 0.97 len(alger_samples) 48 n_girls 41 n_boys 7  -->  girls/n samples 0.85\n",
      "accuracy 0.97 len(alger_samples) 48 n_girls 40 n_boys 8  -->  girls/n samples 0.83\n",
      "accuracy 0.97 len(alger_samples) 48 n_girls 38 n_boys 10  -->  girls/n samples 0.79\n",
      "accuracy 0.96 len(alger_samples) 48 n_girls 37 n_boys 11  -->  girls/n samples 0.77\n",
      "accuracy 0.94 len(alger_samples) 48 n_girls 44 n_boys 4  -->  girls/n samples 0.92\n",
      "accuracy 0.96 len(alger_samples) 48 n_girls 37 n_boys 11  -->  girls/n samples 0.77\n",
      "accuracy 0.96 len(alger_samples) 48 n_girls 42 n_boys 6  -->  girls/n samples 0.88\n",
      "accuracy 0.97 len(alger_samples) 48 n_girls 39 n_boys 9  -->  girls/n samples 0.81\n",
      "accuracy 0.97 len(alger_samples) 48 n_girls 39 n_boys 9  -->  girls/n samples 0.81\n",
      "accuracy 0.97 len(alger_samples) 48 n_girls 42 n_boys 6  -->  girls/n samples 0.88\n",
      "accuracy 0.98 len(alger_samples) 48 n_girls 39 n_boys 9  -->  girls/n samples 0.81\n",
      "accuracy 0.96 len(alger_samples) 48 n_girls 31 n_boys 17  -->  girls/n samples 0.65\n",
      "accuracy 0.96 len(alger_samples) 48 n_girls 35 n_boys 13  -->  girls/n samples 0.73\n",
      "accuracy 0.96 len(alger_samples) 48 n_girls 38 n_boys 10  -->  girls/n samples 0.79\n",
      "accuracy 0.94 len(alger_samples) 48 n_girls 41 n_boys 7  -->  girls/n samples 0.85\n",
      "accuracy 0.96 len(alger_samples) 48 n_girls 39 n_boys 9  -->  girls/n samples 0.81\n",
      "accuracy 0.96 len(alger_samples) 48 n_girls 40 n_boys 8  -->  girls/n samples 0.83\n",
      "accuracy 0.99 len(alger_samples) 48 n_girls 42 n_boys 6  -->  girls/n samples 0.88\n",
      "accuracy 0.98 len(alger_samples) 48 n_girls 40 n_boys 8  -->  girls/n samples 0.83\n",
      "accuracy 0.99 len(alger_samples) 48 n_girls 41 n_boys 7  -->  girls/n samples 0.85\n",
      "accuracy 0.96 len(alger_samples) 48 n_girls 38 n_boys 10  -->  girls/n samples 0.79\n",
      "accuracy 0.97 len(alger_samples) 48 n_girls 33 n_boys 15  -->  girls/n samples 0.69\n",
      "accuracy 0.96 len(alger_samples) 48 n_girls 42 n_boys 6  -->  girls/n samples 0.88\n",
      "accuracy 0.97 len(alger_samples) 48 n_girls 39 n_boys 9  -->  girls/n samples 0.81\n"
     ]
    }
   ],
   "source": [
    "for a in range(25):\n",
    "    \n",
    "    training_sources, training_samples, training_labels = get_some_data(['boys', 'girls'], 2000)\n",
    "    testing_sources, testing_samples, testing_labels = get_some_data(['boys', 'girls'], 2000)\n",
    "    \n",
    "    alger_sources, alger_samples, alger_labels = get_some_data(['alger',], 2000)\n",
    "\n",
    "    dictionary, classifier, score = train_and_test_a_classifier(training_samples, \n",
    "                                                                training_labels, \n",
    "                                                                testing_samples, \n",
    "                                                                testing_labels, \n",
    "                                                                alger_samples)\n",
    "    \n",
    "    alger_corpus = [dictionary.doc2bow(text) for text in alger_samples]\n",
    "\n",
    "    alger_matrix = matutils.corpus2dense(alger_corpus, len(dictionary))\n",
    "    alger_matrix = alger_matrix.T\n",
    "    \n",
    "    results = classifier.predict(alger_matrix)\n",
    "    \n",
    "    n_girls = 0\n",
    "    n_boys = 0\n",
    "    for r in results:\n",
    "        if r == 'girls':\n",
    "            n_girls += 1\n",
    "        if r == 'boys':\n",
    "            n_boys += 1\n",
    "            \n",
    "    print 'accuracy', '%.2f' % score, 'len(alger_samples)', len(alger_samples), \\\n",
    "            'n_girls', n_girls, 'n_boys', n_boys, \\\n",
    "            ' --> ', \\\n",
    "            'girls/n samples', '%.2f' % (float(n_girls) / float(len(alger_samples)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Are these accuracy numbers reasonable computed the way I think they are?\n",
    "\n",
    "Given that I have 157 samples (69 boys + 88 girls) is it possible to compute these accuracy numbers (i.e., number correct / number of samples).  Or are is there something else going on with \"accuracy\"?\n",
    "\n",
    "NLTK (several cells above) returned \n",
    "\n",
    "    accuracy 0.929936305732 which makes sense (11 wrong out of 157)\n",
    "\n",
    "sklearn (immediately preceeding cell) returned:\n",
    "\n",
    "    accuracy 0.949044585987 (8 wrong)\n",
    "    accuracy 0.987261146497 (2)\n",
    "    accuracy 0.955414012739 (7)\n",
    "    accuracy 0.96178343949 (6)\n",
    "    accuracy 0.974522292994 (4)\n",
    "    accuracy 0.96178343949 (6)\n",
    "    \n",
    "The \"accuracy\" numbers look explicable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 wrong =  0.993630573248\n",
      "2 wrong =  0.987261146497\n",
      "3 wrong =  0.980891719745\n",
      "4 wrong =  0.974522292994\n",
      "5 wrong =  0.968152866242\n",
      "6 wrong =  0.96178343949\n",
      "7 wrong =  0.955414012739\n",
      "8 wrong =  0.949044585987\n",
      "9 wrong =  0.942675159236\n",
      "10 wrong =  0.936305732484\n",
      "11 wrong =  0.929936305732\n"
     ]
    }
   ],
   "source": [
    "for a in range(1, 12):\n",
    "    print a, 'wrong = ', float(157 - a) / 157.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Same thing, but for best sellers instead of Alger\n",
    "\n",
    "A slightly different question: **Are best sellers more like texts favored by boys, or by girls?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.96 len(best_seller_matrix) 105 n_girls 95 n_boys 10  -->  girls/n samples 0.90\n",
      "accuracy 0.94 len(best_seller_matrix) 105 n_girls 98 n_boys 7  -->  girls/n samples 0.93\n",
      "accuracy 0.97 len(best_seller_matrix) 105 n_girls 97 n_boys 8  -->  girls/n samples 0.92\n",
      "accuracy 0.96 len(best_seller_matrix) 105 n_girls 94 n_boys 11  -->  girls/n samples 0.90\n",
      "accuracy 0.97 len(best_seller_matrix) 105 n_girls 97 n_boys 8  -->  girls/n samples 0.92\n",
      "accuracy 0.96 len(best_seller_matrix) 105 n_girls 97 n_boys 8  -->  girls/n samples 0.92\n",
      "accuracy 0.97 len(best_seller_matrix) 105 n_girls 95 n_boys 10  -->  girls/n samples 0.90\n",
      "accuracy 0.95 len(best_seller_matrix) 105 n_girls 96 n_boys 9  -->  girls/n samples 0.91\n",
      "accuracy 0.97 len(best_seller_matrix) 105 n_girls 95 n_boys 10  -->  girls/n samples 0.90\n",
      "accuracy 0.92 len(best_seller_matrix) 105 n_girls 97 n_boys 8  -->  girls/n samples 0.92\n",
      "accuracy 0.97 len(best_seller_matrix) 105 n_girls 98 n_boys 7  -->  girls/n samples 0.93\n",
      "accuracy 0.96 len(best_seller_matrix) 105 n_girls 95 n_boys 10  -->  girls/n samples 0.90\n",
      "accuracy 0.95 len(best_seller_matrix) 105 n_girls 97 n_boys 8  -->  girls/n samples 0.92\n",
      "accuracy 0.97 len(best_seller_matrix) 105 n_girls 95 n_boys 10  -->  girls/n samples 0.90\n",
      "accuracy 0.98 len(best_seller_matrix) 105 n_girls 95 n_boys 10  -->  girls/n samples 0.90\n",
      "accuracy 0.98 len(best_seller_matrix) 105 n_girls 95 n_boys 10  -->  girls/n samples 0.90\n",
      "accuracy 0.96 len(best_seller_matrix) 105 n_girls 95 n_boys 10  -->  girls/n samples 0.90\n",
      "accuracy 0.97 len(best_seller_matrix) 105 n_girls 94 n_boys 11  -->  girls/n samples 0.90\n",
      "accuracy 0.94 len(best_seller_matrix) 105 n_girls 91 n_boys 14  -->  girls/n samples 0.87\n",
      "accuracy 0.98 len(best_seller_matrix) 105 n_girls 93 n_boys 12  -->  girls/n samples 0.89\n",
      "accuracy 0.97 len(best_seller_matrix) 105 n_girls 94 n_boys 11  -->  girls/n samples 0.90\n",
      "accuracy 0.97 len(best_seller_matrix) 105 n_girls 95 n_boys 10  -->  girls/n samples 0.90\n",
      "accuracy 0.96 len(best_seller_matrix) 105 n_girls 97 n_boys 8  -->  girls/n samples 0.92\n",
      "accuracy 0.96 len(best_seller_matrix) 105 n_girls 95 n_boys 10  -->  girls/n samples 0.90\n",
      "accuracy 0.96 len(best_seller_matrix) 105 n_girls 95 n_boys 10  -->  girls/n samples 0.90\n"
     ]
    }
   ],
   "source": [
    "for a in range(25):\n",
    "    \n",
    "    training_sources, training_samples, training_labels = get_some_data(['boys', 'girls'], 2000)\n",
    "    testing_sources, testing_samples, testing_labels = get_some_data(['boys', 'girls'], 2000)\n",
    "    \n",
    "    best_seller_sources, best_seller_samples, best_seller_labels = get_some_data(['best_sellers',], 2000)\n",
    "\n",
    "    dictionary, classifier, score = train_and_test_a_classifier(training_samples, \n",
    "                                                                training_labels, \n",
    "                                                                testing_samples, \n",
    "                                                                testing_labels, \n",
    "                                                                best_seller_samples)\n",
    "    \n",
    "    best_seller_corpus = [dictionary.doc2bow(text) for text in best_seller_samples]\n",
    "\n",
    "    best_seller_matrix = matutils.corpus2dense(best_seller_corpus, len(dictionary))\n",
    "    best_seller_matrix = best_seller_matrix.T\n",
    "    \n",
    "    results = classifier.predict(best_seller_matrix)\n",
    "    \n",
    "    n_girls = 0\n",
    "    n_boys = 0\n",
    "    for r in results:\n",
    "        if r == 'girls':\n",
    "            n_girls += 1\n",
    "        if r == 'boys':\n",
    "            n_boys += 1\n",
    "            \n",
    "    print 'accuracy', '%.2f' % score, 'len(best_seller_matrix)', len(best_seller_matrix), \\\n",
    "            'n_girls', n_girls, 'n_boys', n_boys, \\\n",
    "            ' --> ', \\\n",
    "            'girls/n samples', '%.2f' % (float(n_girls) / float(len(best_seller_matrix)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What happens if I leave Alger in with \"boys\"?\n",
    "\n",
    "One theory, which we never proved to our satisfaction, was that **Alger \"trains\" adolescent readers to become readers of adult bestsellers**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.97 len(best_seller_matrix) 105 n_girls 69 n_boys 36  -->  girls/n samples 0.66\n",
      "accuracy 0.94 len(best_seller_matrix) 105 n_girls 69 n_boys 36  -->  girls/n samples 0.66\n",
      "accuracy 0.96 len(best_seller_matrix) 105 n_girls 65 n_boys 40  -->  girls/n samples 0.62\n",
      "accuracy 0.94 len(best_seller_matrix) 105 n_girls 68 n_boys 37  -->  girls/n samples 0.65\n",
      "accuracy 0.94 len(best_seller_matrix) 105 n_girls 70 n_boys 35  -->  girls/n samples 0.67\n",
      "accuracy 0.96 len(best_seller_matrix) 105 n_girls 70 n_boys 35  -->  girls/n samples 0.67\n",
      "accuracy 0.95 len(best_seller_matrix) 105 n_girls 57 n_boys 48  -->  girls/n samples 0.54\n",
      "accuracy 0.96 len(best_seller_matrix) 105 n_girls 68 n_boys 37  -->  girls/n samples 0.65\n",
      "accuracy 0.96 len(best_seller_matrix) 105 n_girls 64 n_boys 41  -->  girls/n samples 0.61\n",
      "accuracy 0.94 len(best_seller_matrix) 105 n_girls 66 n_boys 39  -->  girls/n samples 0.63\n",
      "accuracy 0.96 len(best_seller_matrix) 105 n_girls 66 n_boys 39  -->  girls/n samples 0.63\n",
      "accuracy 0.97 len(best_seller_matrix) 105 n_girls 64 n_boys 41  -->  girls/n samples 0.61\n",
      "accuracy 0.94 len(best_seller_matrix) 105 n_girls 66 n_boys 39  -->  girls/n samples 0.63\n",
      "accuracy 0.96 len(best_seller_matrix) 105 n_girls 67 n_boys 38  -->  girls/n samples 0.64\n",
      "accuracy 0.94 len(best_seller_matrix) 105 n_girls 64 n_boys 41  -->  girls/n samples 0.61\n",
      "accuracy 0.97 len(best_seller_matrix) 105 n_girls 70 n_boys 35  -->  girls/n samples 0.67\n",
      "accuracy 0.95 len(best_seller_matrix) 105 n_girls 66 n_boys 39  -->  girls/n samples 0.63\n",
      "accuracy 0.97 len(best_seller_matrix) 105 n_girls 65 n_boys 40  -->  girls/n samples 0.62\n",
      "accuracy 0.96 len(best_seller_matrix) 105 n_girls 70 n_boys 35  -->  girls/n samples 0.67\n",
      "accuracy 0.97 len(best_seller_matrix) 105 n_girls 68 n_boys 37  -->  girls/n samples 0.65\n",
      "accuracy 0.94 len(best_seller_matrix) 105 n_girls 75 n_boys 30  -->  girls/n samples 0.71\n",
      "accuracy 0.95 len(best_seller_matrix) 105 n_girls 63 n_boys 42  -->  girls/n samples 0.60\n",
      "accuracy 0.97 len(best_seller_matrix) 105 n_girls 64 n_boys 41  -->  girls/n samples 0.61\n",
      "accuracy 0.97 len(best_seller_matrix) 105 n_girls 64 n_boys 41  -->  girls/n samples 0.61\n",
      "accuracy 0.97 len(best_seller_matrix) 105 n_girls 67 n_boys 38  -->  girls/n samples 0.64\n"
     ]
    }
   ],
   "source": [
    "for a in range(25):\n",
    "    \n",
    "    training_sources, training_samples, training_labels = get_some_data(['alger_boys', 'girls'], 2000)\n",
    "    testing_sources, testing_samples, testing_labels = get_some_data(['alger_boys', 'girls'], 2000)\n",
    "    \n",
    "    best_seller_sources, best_seller_samples, best_seller_labels = get_some_data(['best_sellers',], 2000)\n",
    "\n",
    "    dictionary, classifier, score = train_and_test_a_classifier(training_samples, \n",
    "                                                                training_labels, \n",
    "                                                                testing_samples, \n",
    "                                                                testing_labels, \n",
    "                                                                best_seller_samples)\n",
    "    \n",
    "    best_seller_corpus = [dictionary.doc2bow(text) for text in best_seller_samples]\n",
    "\n",
    "    best_seller_matrix = matutils.corpus2dense(best_seller_corpus, len(dictionary))\n",
    "    best_seller_matrix = best_seller_matrix.T\n",
    "    \n",
    "    results = classifier.predict(best_seller_matrix)\n",
    "    \n",
    "    n_girls = 0\n",
    "    n_boys = 0\n",
    "    for r in results:\n",
    "        if r == 'girls':\n",
    "            n_girls += 1\n",
    "        if r == 'alger_boys':\n",
    "            n_boys += 1\n",
    "            \n",
    "    print 'accuracy', '%.2f' % score, 'len(best_seller_matrix)', len(best_seller_matrix), \\\n",
    "            'n_girls', n_girls, 'n_boys', n_boys, \\\n",
    "            ' --> ', \\\n",
    "            'girls/n samples', '%.2f' % (float(n_girls) / float(len(best_seller_matrix)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
