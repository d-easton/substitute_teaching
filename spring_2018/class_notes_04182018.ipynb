{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red;font-weight:bold;font-size: 150%\">These notes are a substantial revision of the notes from April 11, 2018.</span>\n",
    "\n",
    "## textacy, with the Inaugural Addresses . . . \n",
    "\n",
    "[textacy Quick Start](https://chartbeat-labs.github.io/textacy/getting_started/quickstart.html)\n",
    "\n",
    "[textacy API documentation](https://chartbeat-labs.github.io/textacy/api_reference.html#)\n",
    "\n",
    "[textacy github repo](https://github.com/chartbeat-labs/textacy)\n",
    "\n",
    "\n",
    "## . . . and topic modeling . . . \n",
    "\n",
    "**[Mallet](http://mallet.cs.umass.edu/), the java topic modeling package we use most often.**  Note: we have a super-easy web interface for Mallet.\n",
    "\n",
    "[David Mimno explains Topic Modeling](https://vimeo.com/53080123).  Mimno is the maintainer of Mallet.\n",
    "\n",
    "[Ben Schmidt applies topic modeling to ship logs](http://sappingattention.blogspot.com/2012/11/when-you-have-mallet-everything-looks.html)\n",
    "\n",
    "[Scott Weingart, \"Topic Modeling for Humanists: A Guided Tour\"](http://www.scottbot.net/HIAL/index.html@p=19113.html)\n",
    "\n",
    "[Mining the Dispatch](http://dsl.richmond.edu/dispatch/), an exemplary application of topic modeling to a set of historical newspaper data.\n",
    "\n",
    "[My toy topic modeller](https://talus.artsci.wustl.edu/malletTalk/toyTopicModeller.py), written in python.\n",
    "\n",
    "[My github repo for \"understanding_mallet\"](https://github.com/spenteco/understanding_mallet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load textact and spacy . . . \n",
    "\n",
    ". . . check their versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6.0\n",
      "2.0.11\n"
     ]
    }
   ],
   "source": [
    "import textacy, spacy\n",
    "\n",
    "print textacy.__version__\n",
    "print spacy.__version__\n",
    "\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Inaugural Address corpus\n",
    "\n",
    "The \"metadata\" about the addresses are \"buried\" in the file name . . . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access corpora/inaugural_addresses: No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!ls corpora/inaugural_addresses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dig out the metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "metadata = []\n",
    "\n",
    "for path_to_file in glob.glob('corpora/inaugural_addresses/*.txt'):\n",
    "    \n",
    "    address_n = int(path_to_file.split('/')[-1].split('_')[0])\n",
    "    address_year = int(path_to_file.split('/')[-1].split('_')[-1].replace('.txt', ''))\n",
    "    president = '_'.join(path_to_file.split('/')[-1].split('_')[1:-1])\n",
    "    \n",
    "    metadata.append({'address_n': address_n, 'address_year': address_year, 'president': president, 'path_to_file': path_to_file})\n",
    "    \n",
    "metadata.sort(key=lambda address: address['address_n'])\n",
    "\n",
    "for m in metadata[:5]:\n",
    "    print m\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a spacy corpus\n",
    "\n",
    "https://chartbeat-labs.github.io/textacy/getting_started/quickstart.html#working-with-many-texts\n",
    "\n",
    "https://chartbeat-labs.github.io/textacy/api_reference.html#module-textacy.corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(corpus) 0\n",
      "\n",
      "<class 'textacy.corpus.Corpus'>\n",
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-9190f9ef176b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/spenteco/anaconda2/lib/python2.7/site-packages/textacy/corpus.pyc\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx_or_slice)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx_or_slice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx_or_slice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__delitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx_or_slice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import codecs, re\n",
    "\n",
    "corpus = textacy.Corpus(\n",
    "            u'en', \n",
    "            texts = [unicode(re.sub('\\s+', ' ', codecs.open(m['path_to_file'], 'r', encoding='utf-8').read())) for m in metadata],\n",
    "            metadatas = metadata)\n",
    "\n",
    "print 'len(corpus)', len(corpus)\n",
    "\n",
    "print\n",
    "print type(corpus)\n",
    "\n",
    "print\n",
    "print type(corpus[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting the resulting textacy corpus . . . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in range(0, 5):\n",
    "    print corpus.docs[a].metadata, corpus.docs[a]\n",
    "    \n",
    "print\n",
    "print 'docs in corpus', corpus.n_docs\n",
    "print 'sentences in corpus', corpus.n_sents\n",
    "print 'tokens in corpus', corpus.n_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wc -w corpora/inaugural_addresses/1_*\n",
    "!wc -w corpora/inaugural_addresses/2_*\n",
    "!wc -w corpora/inaugural_addresses/3_*\n",
    "!wc -w corpora/inaugural_addresses/4_*\n",
    "!wc -w corpora/inaugural_addresses/5_*\n",
    "\n",
    "!wc -w corpora/inaugural_addresses/* | grep total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keywords, too easy.\n",
    "\n",
    "Note the strange import.  Simply importing \"textacy\" doesn't work for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import textwrap\n",
    "import textacy.keyterms\n",
    "\n",
    "for doc in corpus[:5]:\n",
    "    \n",
    "    print\n",
    "    print doc.metadata['address_year'], doc.metadata['president']\n",
    "    print\n",
    "    \n",
    "    top_words = []\n",
    "    for w in textacy.keyterms.textrank(doc, n_keyterms=20):\n",
    "        top_words.append(w[0])\n",
    "    \n",
    "    print '\\n'.join(textwrap.wrap(', '.join(top_words), 80))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textacy.text_stats import *\n",
    "\n",
    "for doc in corpus[-5:]:\n",
    "\n",
    "    print\n",
    "    print doc.metadata['address_year'], doc.metadata['president']\n",
    "    print\n",
    "    \n",
    "    ts = TextStats(doc)\n",
    "    print 'flesch_kincaid_grade_level', ts.readability_stats['flesch_kincaid_grade_level']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus to document-term matrix . . . \n",
    "\n",
    "Lots of settings.  The doc is pretty good:\n",
    "\n",
    "https://chartbeat-labs.github.io/textacy/getting_started/quickstart.html#analyze-a-corpus\n",
    "\n",
    "https://chartbeat-labs.github.io/textacy/api_reference.html#vectorizers\n",
    "\n",
    "(Note that in the API, see [**textacy.vsm.vectorizers.Vectorizer**](https://chartbeat-labs.github.io/textacy/api_reference.html#textacy.vsm.vectorizers.Vectorizer).)\n",
    "\n",
    "Also, **please see [the to_terms_list doc](https://chartbeat-labs.github.io/textacy/api_reference.html#textacy.doc.Doc.to_terms_list).**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# \"apply_dl\" MEANS \"apply document length\"\n",
    "\n",
    "# FOR TD-IDF WEIGHTS\n",
    "#vectorizer = textacy.Vectorizer(tf_type='linear', apply_idf=True, apply_dl=True)\n",
    "\n",
    "# FOR RAW WORD COUNTS\n",
    "#vectorizer = textacy.Vectorizer(tf_type='linear', apply_idf=False, apply_dl=False)\n",
    "\n",
    "# FOR RELATIVE FREQUENCY . . . \n",
    "vectorizer = textacy.Vectorizer(tf_type='linear', apply_idf=False, apply_dl=False, min_df=2, max_df=0.90)\n",
    "\n",
    "doc_term_matrix = vectorizer.fit_transform((doc.to_terms_list(ngrams=1, \n",
    "                                                                named_entities=True,\n",
    "                                                                normalize='lemma',\n",
    "                                                                filter_stops=True,\n",
    "                                                                filter_punct=True,\n",
    "                                                                filter_nums=True,\n",
    "                                                                as_strings=True) \n",
    "                                               for doc in corpus))\n",
    "\n",
    "print\n",
    "print repr(doc_term_matrix)\n",
    "print\n",
    "print doc_term_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect the document-term matrix\n",
    "\n",
    "Convert the sparse matrix to a dense matrix (i.e., one with all the zeros).\n",
    "\n",
    "Inspect in various ways.  Does it look reasonable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dense_doc_term_matrix = doc_term_matrix.todense()\n",
    "\n",
    "print\n",
    "print repr(dense_doc_term_matrix)\n",
    "print\n",
    "print dense_doc_term_matrix.shape\n",
    "\n",
    "list_doc_term_matrix = dense_doc_term_matrix.tolist()\n",
    "\n",
    "print\n",
    "print list_doc_term_matrix[0][:100]\n",
    "print\n",
    "print len(list_doc_term_matrix), len(list_doc_term_matrix[0])\n",
    "print\n",
    "for a in range(len(list_doc_term_matrix[0][:750])):\n",
    "    if list_doc_term_matrix[0][a] > 0:\n",
    "        #print vectorizer.id_to_term[a], list_doc_term_matrix[0][a], ';',\n",
    "        print vectorizer.id_to_term[a],\n",
    "print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Let's do some actual text analysis\n",
    "\n",
    "We do two things here:\n",
    "\n",
    "1.  Create a vectorizer, then use it to create a document-term matrix.\n",
    "2.  Topic model using the document-term matrix.\n",
    "3.  List the words associated with each resulting topic.\n",
    "\n",
    "Lots of experimentation with Vectorizer parameters.  Raw word counts seem to work best.\n",
    "\n",
    "And lots of experimentation with \"n_topics\" in creating the topic model.  20 seemed<br/>\n",
    "reasonable for this demonstration.\n",
    "\n",
    "### The docs:\n",
    "\n",
    "https://chartbeat-labs.github.io/textacy/getting_started/quickstart.html#analyze-a-corpus\n",
    "\n",
    "https://chartbeat-labs.github.io/textacy/api_reference.html#topic-models\n",
    "\n",
    "Tuning the topic model requires [a visit to the sklearn site](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR TD-IDF WEIGHTS\n",
    "#vectorizer = textacy.Vectorizer(tf_type='linear', apply_idf=True, apply_dl=True,  min_df=2, max_df=25)\n",
    "\n",
    "# FOR RAW WORD COUNTS\n",
    "#vectorizer = textacy.Vectorizer(tf_type='linear', apply_idf=False, apply_dl=False,  min_df=2, max_df=40)\n",
    "#vectorizer = textacy.Vectorizer(tf_type='linear', apply_idf=False, apply_dl=False)\n",
    "\n",
    "# FOR RELATIVE FREQUENCY\n",
    "#vectorizer = textacy.Vectorizer(tf_type='linear', apply_idf=False, apply_dl=True,  min_df=2, max_df=50)\n",
    "\n",
    "# FOR RAW WORD COUNT.   LOW max_df VALUE REMOVES THINGS LIKE 'government' AND 'america' ANd SEEMS TO GIVE THE BEST RESULT\n",
    "vectorizer = textacy.Vectorizer(tf_type='linear', apply_idf=False, apply_dl=False,  min_df=5, max_df=30)\n",
    "\n",
    "doc_term_matrix = vectorizer.fit_transform([doc.to_terms_list(ngrams=1,\n",
    "                                                                named_entities=True,\n",
    "                                                                normalize='lemma',\n",
    "                                                                filter_stops=True,\n",
    "                                                                filter_punct=True,\n",
    "                                                                filter_nums=True,\n",
    "                                                                as_strings=True) \n",
    "                                               for doc in corpus])\n",
    "\n",
    "model = textacy.TopicModel('lda', n_topics=15)\n",
    "model.fit(doc_term_matrix)\n",
    "\n",
    "doc_topic_matrix = model.transform(doc_term_matrix)\n",
    "\n",
    "print \n",
    "for topic_idx, top_terms in model.top_topic_terms(vectorizer.id_to_term, top_n=20):\n",
    "    print 'topic', topic_idx, ':', ' '.join(top_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!grep -li law corpora/inaugural_addresses/* | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the document-topic percentages?\n",
    "\n",
    "I.e., which topics make up what percentage of each document?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "import tabletext\n",
    "\n",
    "def make_printable(topic_pcts):\n",
    "    printable_pcts = []\n",
    "    for pct in topic_pcts:\n",
    "        formatted_pct = '%.2f' % pct\n",
    "        if formatted_pct == '0.00':\n",
    "            formatted_pct = '    '\n",
    "        printable_pcts.append(formatted_pct)\n",
    "    return printable_pcts\n",
    "\n",
    "topic_headings = []\n",
    "for a in range(len(doc_topic_matrix[0])):\n",
    "    topic_headings.append(str(a).rjust(5))\n",
    "    \n",
    "results =[['', '', ''] + topic_headings]\n",
    "    \n",
    "for a in range(len(doc_topic_matrix)):\n",
    "    results.append([a, corpus[a].metadata['address_year'], corpus[a].metadata['president'][:15]] + make_printable(doc_topic_matrix[a]))\n",
    "    \n",
    "print\n",
    "print tabletext.to_text(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List the words associated with each topic\n",
    "\n",
    "We did this once.  I do it again, because I want to see more words, and I'd<br/>\n",
    "like something that doesn't result in a wide display."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "print\n",
    "\n",
    "print \n",
    "for topic_idx, top_terms in model.top_topic_terms(vectorizer.id_to_term, top_n=100):\n",
    "    print\n",
    "    print 'topic', topic_idx, ':', '\\n'.join(textwrap.wrap(' '.join(top_terms), 80))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
